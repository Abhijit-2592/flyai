{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVaVbuBybM3Wh2Ak1GWDKK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhijit-2592/flyai/blob/main/notebooks/lec01_understanding_quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Neural Network Quantization\n",
        "Neural network quantization is the process of reducing the precision of the numerical values used in a neural network, typically the weights and activations, to make the model more efficient in terms of memory usage, computational speed, and energy consumption. Instead of using 32-bit floating-point (FP) numbers, quantization reduces them to lower precision formats like 8-bit integers. This can lead to faster inference and reduced power consumption. However, quantization may result in a small loss of accuracy, though techniques like quantization-aware training help minimize this effect.\n",
        "\n",
        "There are two types of Neural Network Quantization:\n",
        "1. Symmetric Quantization\n",
        "2. Affine/Asymmetric Quantization\n",
        "\n",
        "\n",
        "## Symmetric Quantization\n",
        "In symmetric quantization, the range of the original FP values is mapped to a symmetric range around zero in the quantized space. Thus 0 in FP is mapped to 0 in the quantized space.\n",
        "\n",
        "![Symmetric Quantization Image](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F730bbb8a-3a44-47f6-aefe-f652b117ae22_1124x600.png)\n",
        "\n",
        "Image/text Courtesy: [A Visual Guide To Quantization](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization) by [Maarten Grootendorst](https://www.linkedin.com/in/mgrootendorst)\n"
      ],
      "metadata": {
        "id": "GFNl4R2wxClX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "from dataclasses import dataclass"
      ],
      "metadata": {
        "id": "vPrp0lRL1H5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = torch.manual_seed(555)"
      ],
      "metadata": {
        "id": "iBIa6WsJ50gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class QuantizedTensor:\n",
        "    qtensor: torch.Tensor\n",
        "    scale: torch.Tensor\n",
        "    orig_dtype: torch.dtype\n",
        "    zero_point: torch.Tensor = torch.tensor(0.0)"
      ],
      "metadata": {
        "id": "subtAap-5Q8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SymmetricQuantizer:\n",
        "    def __init__(self, nbits:int = 8):\n",
        "        self.nbits = nbits\n",
        "        self.q_min = -math.pow(2, nbits - 1)  # This is not used\n",
        "        self.q_max = math.pow(2, nbits - 1) - 1\n",
        "\n",
        "    @property\n",
        "    def q_range(self):\n",
        "        return f\"[-{self.q_max}, {self.q_max}]\"\n",
        "\n",
        "    def _calculate_scales(self, tensor: torch.Tensor) -> torch.Tensor:\n",
        "        scale = torch.amax(torch.abs(tensor))/self.q_max\n",
        "        return scale.to(torch.float32)\n",
        "\n",
        "    def quantize(self, tensor:torch.Tensor, q_dtype:torch.dtype = torch.int8) -> QuantizedTensor:\n",
        "        scale = self._calculate_scales(tensor)\n",
        "        qtensor = torch.round(tensor/scale)\n",
        "        qtensor = torch.clamp(qtensor, min=-self.q_max, max=self.q_max)\n",
        "        return QuantizedTensor(qtensor=qtensor.to(q_dtype), scale=scale, orig_dtype=tensor.dtype)\n",
        "\n",
        "    def dequantize(self, qtensor:QuantizedTensor):\n",
        "        tensor = qtensor.qtensor * qtensor.scale\n",
        "        return tensor.to(qtensor.orig_dtype)\n",
        "\n",
        "    def calculate_quantization_mae(self, tensor:torch.Tensor, q_dtype:torch.dtype = torch.int8):\n",
        "        reconstructed_tensor = self.dequantize(self.quantize(tensor, q_dtype=q_dtype))\n",
        "        print(\"Reconstructed Tensor\")\n",
        "        print(reconstructed_tensor)\n",
        "        return torch.mean(torch.abs(tensor-reconstructed_tensor))"
      ],
      "metadata": {
        "id": "BREWFHJH1R80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a random uniform tensor of shape (5,5) between [-5,10)\n",
        "# Uniform(r1,r2) = (r1-r2)*Uniform(0,1) + r2\n",
        "a = -15*torch.rand(5, 5) + 10\n",
        "# Manually make an element to 0.0 so that we can see how the SymmetricQuantizer maps 0\n",
        "a[2][3] = 0.0\n",
        "print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfoSY7ab5p_a",
        "outputId": "57b2072d-dbd0-4371-f0cb-f48b2cb2ef74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.4829,  3.1988,  6.2363,  7.0474, -1.7724],\n",
            "        [ 9.5538, -0.6993, -2.4891, -1.8361,  3.5922],\n",
            "        [ 2.1141,  3.3869,  7.6704,  0.0000,  9.4361],\n",
            "        [ 7.8068, -2.4722, -2.1322, -4.3650, -2.4265],\n",
            "        [ 4.2931,  0.1434, -2.4976,  2.3279,  9.4024]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Symmetric Quantization\")\n",
        "print(\"Tensor\")\n",
        "print(a)\n",
        "symmetric_quantizer = SymmetricQuantizer(nbits=8)\n",
        "qtensor = symmetric_quantizer.quantize(a)\n",
        "print(\"Quantized Tensor\")\n",
        "print(qtensor.qtensor)\n",
        "print(f\"Scale: {qtensor.scale}\")\n",
        "print(f\"Quantization Range: {symmetric_quantizer.q_range}\")\n",
        "print(f\"Quantization Mean Absolute Error: {symmetric_quantizer.calculate_quantization_mae(a)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sLhb3N16UI2",
        "outputId": "67a9031d-623e-4ca0-8bae-b79f44395091"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor\n",
            "tensor([[-1.4829,  3.1988,  6.2363,  7.0474, -1.7724],\n",
            "        [ 9.5538, -0.6993, -2.4891, -1.8361,  3.5922],\n",
            "        [ 2.1141,  3.3869,  7.6704,  0.0000,  9.4361],\n",
            "        [ 7.8068, -2.4722, -2.1322, -4.3650, -2.4265],\n",
            "        [ 4.2931,  0.1434, -2.4976,  2.3279,  9.4024]])\n",
            "Quantized Tensor\n",
            "tensor([[-20,  43,  83,  94, -24],\n",
            "        [127,  -9, -33, -24,  48],\n",
            "        [ 28,  45, 102,   0, 125],\n",
            "        [104, -33, -28, -58, -32],\n",
            "        [ 57,   2, -33,  31, 125]], dtype=torch.int8)\n",
            "Scale: 0.07522675395011902\n",
            "Quantization Range: [-127.0, 127.0]\n",
            "Quantization Mean Absolute Error: 0.014071819372475147\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can clearly see:\n",
        "- 0 is mapped to 0.\n",
        "- The highest value is mapped to +127.\n",
        "- The min value (-ve of the abs(highest value) if present) will be mapped to -127. This is because we are using torch.amax(tensor.abs()) to calculate the scale\n",
        "- Here we aren't using the quantization range effectively."
      ],
      "metadata": {
        "id": "2WU_bNAd9jng"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Asymmetric Quantization\n",
        "Asymmetric quantization, in contrast, is not symmetric around zero. Instead, it maps the minimum (β) and maximum (α) values from the float range to the minimum and maximum values of the quantized range.\n",
        "\n",
        "![asymmetric quantization](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ffa0c54-88bf-45c1-8636-bdb097bb8e6b_1172x848.png)\n",
        "\n",
        "Image/text courtesy: [A Visual Guide To Quantization](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization) by [Maarten Grootendorst](https://www.linkedin.com/in/mgrootendorst)"
      ],
      "metadata": {
        "id": "pr7FnbfCAcYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AsymmetricQuantizer:\n",
        "    def __init__(self, nbits: int):\n",
        "        self.nbits = nbits\n",
        "        self.qmin = -math.pow(2, self.nbits - 1)\n",
        "        self.qmax = math.pow(2, self.nbits - 1) - 1\n",
        "\n",
        "    @property\n",
        "    def q_range(self):\n",
        "        return f\"[{self.qmin}, {self.qmax}]\"\n",
        "\n",
        "    def _calculate_scale(self, tensor: torch.Tensor) -> torch.Tensor:\n",
        "        rmin, rmax = torch.amin(tensor), torch.amax(tensor)\n",
        "        return (rmax-rmin)/(self.qmax-self.qmin)\n",
        "\n",
        "    def _calculate_zero_point(self, tensor: torch.Tensor, scale: torch.Tensor):\n",
        "        rmin = torch.amin(tensor)\n",
        "        return torch.round(self.qmin - rmin/scale)\n",
        "\n",
        "    def quantize(self, tensor:torch.Tensor, q_dtype:torch.dtype = torch.int8) -> QuantizedTensor:\n",
        "        scale = self._calculate_scale(tensor)\n",
        "        zero_point = self._calculate_zero_point(tensor, scale)\n",
        "        qtensor = torch.clamp(torch.round(tensor/scale) + zero_point, min=self.qmin, max=self.qmax)\n",
        "        return QuantizedTensor(\n",
        "            qtensor = qtensor.to(q_dtype),\n",
        "            scale=scale,\n",
        "            orig_dtype=tensor.dtype,\n",
        "            zero_point=zero_point.to(q_dtype)\n",
        "            )\n",
        "\n",
        "    def dequantize(self, qtensor:QuantizedTensor):\n",
        "        tensor = (qtensor.qtensor.to(qtensor.orig_dtype) - qtensor.zero_point.to(qtensor.orig_dtype)) * qtensor.scale\n",
        "        return tensor.to(qtensor.orig_dtype)\n",
        "\n",
        "    def calculate_quantization_mae(self, tensor:torch.Tensor, q_dtype:torch.dtype = torch.int8):\n",
        "        reconstructed_tensor = self.dequantize(self.quantize(tensor, q_dtype=q_dtype))\n",
        "        print(\"Reconstructed Tensor\")\n",
        "        print(reconstructed_tensor)\n",
        "        return torch.mean(torch.abs(tensor-reconstructed_tensor))"
      ],
      "metadata": {
        "id": "T1cOZ-kX-OSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Asymmetric Quantization\")\n",
        "print(\"Tensor\")\n",
        "print(a)\n",
        "asymmetric_quantizer = AsymmetricQuantizer(nbits=8)\n",
        "qtensor = asymmetric_quantizer.quantize(a)\n",
        "print(\"Quantized Tensor\")\n",
        "print(qtensor.qtensor)\n",
        "print(f\"Scale: {qtensor.scale}\")\n",
        "print(f\"Zero Point: {qtensor.zero_point}\")\n",
        "print(f\"Quantization Range: {asymmetric_quantizer.q_range}\")\n",
        "print(f\"Quantization Mean Absolute Error: {asymmetric_quantizer.calculate_quantization_mae(a)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYFjRJoJCiIx",
        "outputId": "421f2976-a1aa-412e-afe1-84fd3f333ed5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor\n",
            "tensor([[-1.4829,  3.1988,  6.2363,  7.0474, -1.7724],\n",
            "        [ 9.5538, -0.6993, -2.4891, -1.8361,  3.5922],\n",
            "        [ 2.1141,  3.3869,  7.6704,  0.0000,  9.4361],\n",
            "        [ 7.8068, -2.4722, -2.1322, -4.3650, -2.4265],\n",
            "        [ 4.2931,  0.1434, -2.4976,  2.3279,  9.4024]])\n",
            "Quantized Tensor\n",
            "tensor([[ -75,   11,   66,   81,  -80],\n",
            "        [ 127,  -61,  -94,  -82,   18],\n",
            "        [  -9,   14,   93,  -48,  125],\n",
            "        [  95,  -93,  -87, -128,  -92],\n",
            "        [  31,  -45,  -94,   -5,  124]], dtype=torch.int8)\n",
            "Scale: 0.05458369106054306\n",
            "Zero Point: -48\n",
            "Quantization Range: [-128.0, 127.0]\n",
            "Quantization Mean Absolute Error: 0.012920784763991833\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can clearly see:\n",
        "\n",
        "- 0 is mapped to the zero_point.\n",
        "- The highest value is mapped to +127.\n",
        "- The lowest is mapped to -128\n",
        "- The Quantization MAE is smaller than symmetric quantization. This is due to effective utilization of the entire quantization range.\n"
      ],
      "metadata": {
        "id": "oZhwLaUuFUMg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional Notes\n",
        "\n",
        "What we have done in this notebook is the simplest form of quantization called **per tensor quantization** (one scale and zero point per entire tensor). As you can probably guess, we can improve the quantization accuracy by increasing the scales/zero point granularity:\n",
        "- one scale/zero point per channel: **per channel quantization**. This is popular in quantizing **CNN's** weights and inputs/activations\n",
        "- one scale/zero point per token: **per token quantization**. This is popular in quantizing **LLM's** inputs/activations.\n",
        "- one scale/zero point per group of let's say 128 values: **group quantization**. This is popular in quantizing **LLM's** weights.\n",
        "\n",
        "### Tradeoff\n",
        "- As we increase the quantization granularity, the quantization error decreases but we also have to store a larger number of scales and zero points. Thus reducing the memory savings.\n",
        "- Even though we do extra operations (dequantization etc.) we still get increased compute efficiency because, most modern GPUs support hardware accleration for Integer Matmuls. For example [Nvidia A100s](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf) have a **19.5 TFLOPs** for **FP32 Matmuls** as compared to **624 TFLOPS** for **INT8 Matmuls**"
      ],
      "metadata": {
        "id": "8alnjtNdHWH8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qfkmTOBZJcDM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}